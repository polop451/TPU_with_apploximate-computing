# Activation Functions à¸ªà¸³à¸«à¸£à¸±à¸š TPU

## ğŸ¯ Overview

TPU à¸‚à¸­à¸‡à¹€à¸£à¸²à¸£à¸­à¸‡à¸£à¸±à¸š **7 activation functions** à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸à¸±à¸™à¸—à¸±à¹ˆà¸§à¹„à¸›à¹ƒà¸™ Deep Learning à¸à¸£à¹‰à¸­à¸¡à¸—à¸±à¹‰à¸‡ INT8 à¹à¸¥à¸° FP16!

## ğŸ“Š Activation Functions à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š

### 1. ğŸ”¥ **ReLU (Rectified Linear Unit)** - à¸¢à¸­à¸”à¸™à¸´à¸¢à¸¡à¸—à¸µà¹ˆà¸ªà¸¸à¸”!

```
f(x) = max(0, x)
```

**à¸à¸£à¸²à¸Ÿ:**
```
     â”‚
   6 â”‚         â•±
   5 â”‚       â•±
   4 â”‚     â•±
   3 â”‚   â•±
   2 â”‚ â•±
   1 â”‚â•±
   0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  -1 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- âš¡ **à¹€à¸£à¹‡à¸§à¸¡à¸²à¸** - à¹à¸„à¹ˆà¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸à¸±à¸š 0
- ğŸ¯ **à¹„à¸¡à¹ˆà¸¡à¸µ vanishing gradient**
- ğŸ’¾ **à¹ƒà¸Šà¹‰ resource à¸™à¹‰à¸­à¸¢** - à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸„à¸³à¸™à¸§à¸“à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™
- ğŸ† **à¹ƒà¸Šà¹‰à¹ƒà¸™ 90% à¸‚à¸­à¸‡ modern CNNs**

**à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:**
- âš ï¸ "Dying ReLU" - neurons à¸­à¸²à¸ˆà¸•à¸²à¸¢à¸–à¸²à¸§à¸£

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- ResNet, VGG, AlexNet
- Image classification
- Object detection
- à¸—à¸¸à¸à¹† hidden layer à¸‚à¸­à¸‡ CNN à¸ªà¹ˆà¸§à¸™à¹ƒà¸«à¸à¹ˆ

**Code:**
```verilog
// INT8
result = (x < 0) ? 0 : x;

// FP16
result = is_negative(x) ? 0.0 : x;
```

---

### 2. ğŸ“± **ReLU6** - à¸ªà¸³à¸«à¸£à¸±à¸š Mobile Networks

```
f(x) = min(max(0, x), 6)
```

**à¸à¸£à¸²à¸Ÿ:**
```
   6 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   5 â”‚       â•±
   4 â”‚     â•±
   3 â”‚   â•±
   2 â”‚ â•±
   1 â”‚â•±
   0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- ğŸ“± **à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š mobile/embedded**
- ğŸ”¢ **à¸”à¸µà¸•à¹ˆà¸­ quantization** (bounded output)
- âš¡ **à¹€à¸£à¹‡à¸§à¹€à¸—à¹ˆà¸² ReLU**
- ğŸ’¾ **à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¸à¸¥à¸±à¸‡à¸‡à¸²à¸™**

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- **MobileNet** (v1, v2, v3)
- **EfficientNet**
- **Mobile deployment**
- **Edge AI applications**

**Code:**
```verilog
if (x < 0)      result = 0;
else if (x > 6) result = 6;
else            result = x;
```

---

### 3. ğŸ“ˆ **Leaky ReLU**

```
f(x) = x if x > 0, else Î±*x  (Î± = 0.01)
```

**à¸à¸£à¸²à¸Ÿ:**
```
     â”‚
   5 â”‚       â•±
   4 â”‚     â•±
   3 â”‚   â•±
   2 â”‚ â•±
   1 â”‚â•±
   0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  -1 â”‚â•±
  -2 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- âœ… **à¹à¸à¹‰à¸›à¸±à¸à¸«à¸² dying ReLU**
- ğŸ“‰ **à¸¡à¸µ gradient à¹€à¸ªà¸¡à¸­**
- ğŸ¯ **Better than ReLU** à¸ªà¸³à¸«à¸£à¸±à¸šà¸šà¸²à¸‡ tasks

**à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:**
- âš ï¸ à¸•à¹‰à¸­à¸‡à¹€à¸¥à¸·à¸­à¸ Î± à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- GANs (Generative Adversarial Networks)
- à¹€à¸¡à¸·à¹ˆà¸­ ReLU à¸—à¸³à¹ƒà¸«à¹‰ neurons à¸•à¸²à¸¢
- Tasks à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£ negative activation

**Code:**
```verilog
result = (x < 0) ? x * 0.01 : x;
// Hardware: x >> 7 â‰ˆ x * 0.0078 â‰ˆ x * 0.01
```

---

### 4. ã€°ï¸ **Sigmoid**

```
f(x) = 1 / (1 + e^(-x))
Output: (0, 1)
```

**à¸à¸£à¸²à¸Ÿ:**
```
   1 â”‚    â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚   â•±
 0.5 â”‚  â•±
     â”‚ â•±
   0 â”‚â”€â”€â”€â”€â”€â”€â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- ğŸ¯ **Output bounded (0,1)** - à¹€à¸«à¸¡à¸²à¸°à¸—à¸³ probability
- ğŸ“Š **Smooth gradient**

**à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:**
- âš ï¸ **Vanishing gradient** (gradient â‰ˆ 0 à¹€à¸¡à¸·à¹ˆà¸­ x à¸¡à¸²à¸)
- ğŸ’» **Expensive** to compute (e^x)
- ğŸŒ **à¸Šà¹‰à¸²**

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- **Binary classification** output layer
- **LSTM gates** (forget, input, output gates)
- **Attention mechanisms**
- à¹€à¸¡à¸·à¹ˆà¸­à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ output à¹€à¸›à¹‡à¸™ probability

**Hardware Implementation:**
```verilog
// Approximate with piecewise linear
if (x > 4)       result = 1.0;
else if (x < -4) result = 0.0;
else             result = 0.5 + 0.25*x;  // Linear approximation
```

---

### 5. ã€°ï¸ **Tanh (Hyperbolic Tangent)**

```
f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
Output: (-1, 1)
```

**à¸à¸£à¸²à¸Ÿ:**
```
   1 â”‚    â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚   â•±
   0 â”‚  â•±
     â”‚ â•±
  -1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- ğŸ¯ **Zero-centered** (output à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ -1 à¸–à¸¶à¸‡ 1)
- ğŸ“Š **Better than Sigmoid** à¹ƒà¸™à¸šà¸²à¸‡ cases

**à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:**
- âš ï¸ **Vanishing gradient** (à¹€à¸«à¸¡à¸·à¸­à¸™ Sigmoid)
- ğŸ’» **Expensive** to compute

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- **LSTM cells**
- **RNN hidden states**
- à¹€à¸¡à¸·à¹ˆà¸­à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ zero-centered output

**Hardware Implementation:**
```verilog
// Approximate
if (x > 2)       result = 1.0;
else if (x < -2) result = -1.0;
else             result = x;  // Linear in middle
```

---

### 6. ğŸŒŠ **Swish / SiLU**

```
f(x) = x * sigmoid(x)
```

**à¸à¸£à¸²à¸Ÿ:**
```
     â”‚
   6 â”‚         â•±
   5 â”‚       â•±
   4 â”‚     â•±
   3 â”‚   â•±
   2 â”‚ â•±
   1 â”‚â•±
   0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  -1 â”‚ â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- ğŸ¯ **Smoother than ReLU**
- ğŸ“ˆ **Better gradient flow**
- ğŸ† **SOTA results** à¹ƒà¸™à¸«à¸¥à¸²à¸¢ tasks
- âœ… **Self-gating** mechanism

**à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:**
- ğŸ’» **More expensive** than ReLU

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- **EfficientNet** (Google's SOTA architecture)
- **Modern mobile networks**
- **NAS-discovered architectures**

**Code:**
```verilog
result = x * sigmoid(x);
// Approximate: x * (x > 0 ? 1 : 0.1)
```

---

### 7. ğŸ“ **GELU (Gaussian Error Linear Unit)**

```
f(x) = 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715*xÂ³)))
Approximate: f(x) â‰ˆ x * sigmoid(1.702*x)
```

**à¸à¸£à¸²à¸Ÿ:**
```
     â”‚
   6 â”‚         â•±
   5 â”‚       â•±
   4 â”‚     â•±
   3 â”‚   â•±â•±
   2 â”‚ â•±â•±
   1 â”‚â•±
   0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  -1 â”‚ â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- ğŸ¤– **Used in Transformers!** (BERT, GPT)
- ğŸ¯ **Smooth, non-monotonic**
- ğŸ“Š **Probabilistic interpretation**

**à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:**
- ğŸ’» **Very expensive** to compute exactly

**à¹ƒà¸Šà¹‰à¹ƒà¸™:**
- **BERT** (Google's language model)
- **GPT** (OpenAI's models)
- **Transformers** (all modern NLP)
- **Vision Transformers**

**Hardware Implementation:**
```verilog
// Highly simplified
if (x < -2)      result = 0;
else if (x < 0)  result = x * 0.25;
else             result = x;
```

---

## ğŸ“Š Comparison Table

| Activation | Speed | Accuracy | Vanishing Gradient | Use Case | Popularity |
|-----------|-------|----------|-------------------|----------|------------|
| **ReLU** | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | âœ… No | CNN (general) | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **ReLU6** | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | âœ… No | Mobile AI | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **Leaky ReLU** | âš¡âš¡âš¡âš¡ | â­â­â­â­ | âœ… No | GAN, when ReLU fails | ğŸ”¥ğŸ”¥ğŸ”¥ |
| **Sigmoid** | âš¡âš¡ | â­â­â­ | âŒ Yes | Binary output, LSTM | ğŸ”¥ğŸ”¥ |
| **Tanh** | âš¡âš¡ | â­â­â­ | âŒ Yes | LSTM, RNN | ğŸ”¥ğŸ”¥ |
| **Swish** | âš¡âš¡âš¡ | â­â­â­â­â­ | âœ… No | Modern CNNs | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **GELU** | âš¡âš¡ | â­â­â­â­â­ | âœ… No | Transformers, NLP | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |

## ğŸ¯ à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ Activation Function

### à¹€à¸¥à¸·à¸­à¸à¸•à¸²à¸¡ Application:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image Classification (CNN)                     â”‚
â”‚  âœ“ ReLU (hidden layers)                        â”‚
â”‚  âœ“ Softmax (output layer)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mobile/Edge Deployment                         â”‚
â”‚  âœ“ ReLU6 (MobileNet, EfficientNet)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Binary Classification                          â”‚
â”‚  âœ“ ReLU (hidden)                               â”‚
â”‚  âœ“ Sigmoid (output)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RNN / LSTM                                     â”‚
â”‚  âœ“ Tanh (cell state)                           â”‚
â”‚  âœ“ Sigmoid (gates)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformers / NLP (BERT, GPT)                â”‚
â”‚  âœ“ GELU (feedforward layers)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GANs                                          â”‚
â”‚  âœ“ Leaky ReLU (discriminator)                 â”‚
â”‚  âœ“ Tanh (generator output)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOTA Modern CNNs (EfficientNet, etc)          â”‚
â”‚  âœ“ Swish / SiLU                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Hardware Implementation Details

### Resource Usage (per activation unit):

| Activation | LUTs | FFs | DSP | Latency | Notes |
|-----------|------|-----|-----|---------|-------|
| ReLU | ~5 | ~8 | 0 | 1 cycle | Just comparison |
| ReLU6 | ~10 | ~8 | 0 | 1 cycle | 2 comparisons |
| Leaky ReLU | ~20 | ~16 | 0 | 1 cycle | Add shifter |
| Sigmoid | ~100 | ~32 | 0 | 2-3 cycles | Piecewise approx |
| Tanh | ~100 | ~32 | 0 | 2-3 cycles | Piecewise approx |
| Swish | ~120 | ~40 | 0 | 3-4 cycles | ReLU + Sigmoid |
| GELU | ~150 | ~48 | 0 | 3-5 cycles | Complex approx |

### Total for 8x8 Array:

```
ReLU Layer (64 units):
  - LUTs: ~320 (1.5% of Basys3)
  - Very cheap! âœ“

Swish Layer (64 units):
  - LUTs: ~7,680 (37% of Basys3)
  - Still fits! âš ï¸

GELU Layer (64 units):
  - LUTs: ~9,600 (46% of Basys3)
  - Tight but OK! âš ï¸
```

## ğŸ’¡ Best Practices

### 1. **Start Simple:**
```
âœ“ Always try ReLU first
âœ“ Use ReLU6 for mobile
âœ“ Only use complex functions if needed
```

### 2. **Mix and Match:**
```python
# Example network
Input â†’ Conv + ReLU â†’ Conv + ReLU â†’ ... â†’ FC + Sigmoid/Softmax
        ^^^^^^         ^^^^^^               ^^^^^^^^^^^^^^^^
        Hidden layers  Hidden layers        Output layer
        (ReLU)        (ReLU)               (Task-dependent)
```

### 3. **Hardware Considerations:**
```
âœ“ ReLU/ReLU6: Nearly free
âš ï¸ Sigmoid/Tanh: Use approximations
âš ï¸ GELU: Only if transformer is needed
âœ“ Consider LUT-based for better accuracy
```

## ğŸ“ à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™ TPU

### Example 1: Simple CNN
```verilog
// Layer 1: Conv + ReLU
systolic_array conv1(...);
activation_layer #(.activation_type(RELU)) act1(...);

// Layer 2: Conv + ReLU
systolic_array conv2(...);
activation_layer #(.activation_type(RELU)) act2(...);

// Output: FC + Sigmoid
systolic_array fc(...);
activation_layer #(.activation_type(SIGMOID)) act_out(...);
```

### Example 2: MobileNet
```verilog
// Use ReLU6 throughout
activation_layer #(
    .SIZE(8),
    .DATA_WIDTH(16),
    .IS_FLOATING_POINT(1)
) mobilenet_activation (
    .activation_type(RELU6),  // ReLU6 for mobile
    .data_in(conv_output),
    .data_out(activated_output)
);
```

### Example 3: Transformer
```verilog
// Use GELU for feedforward
activation_layer #(
    .activation_type(GELU)
) transformer_ffn (
    .data_in(linear_output),
    .data_out(gelu_output)
);
```

## ğŸ“ˆ Performance Impact

### Accuracy Trade-offs:

```
Network: ResNet-50 on ImageNet

ReLU:           76.5% Top-1 Accuracy
ReLU6:          76.3% Top-1 (-0.2%)  âœ“ Mobile-friendly
Leaky ReLU:     76.7% Top-1 (+0.2%)
Swish:          77.2% Top-1 (+0.7%)  âœ“ Better but slower
GELU:           76.8% Top-1 (+0.3%)  (for transformers)
```

### Speed Comparison (inference time):

```
ReLU:       100%  (baseline) âš¡âš¡âš¡âš¡âš¡
ReLU6:      102%  (+2%)     âš¡âš¡âš¡âš¡âš¡
Leaky ReLU: 105%  (+5%)     âš¡âš¡âš¡âš¡
Sigmoid:    140%  (+40%)    âš¡âš¡âš¡
Swish:      160%  (+60%)    âš¡âš¡âš¡
GELU:       180%  (+80%)    âš¡âš¡
```

## ğŸš€ Advanced Features

### 1. **Dynamic Activation Selection:**
```verilog
// Runtime selection based on layer
always @(*) begin
    if (layer_id < 10)
        activation_type = RELU;      // Hidden layers
    else if (layer_id < 20)
        activation_type = SWISH;     // Later layers
    else
        activation_type = SIGMOID;   // Output
end
```

### 2. **Approximate vs Exact Mode:**
```verilog
parameter APPROXIMATE = 1;  // 1=fast, 0=accurate

if (APPROXIMATE)
    sigmoid_approx(...);  // Piecewise linear
else
    sigmoid_lut(...);     // Lookup table
```

### 3. **Per-Channel Activation:**
```verilog
// Different activation per channel (advanced)
for (i = 0; i < CHANNELS; i++) begin
    if (important_channel[i])
        use_exact_activation();
    else
        use_approximate_activation();
end
```

## ğŸ“š Research Papers

### ReLU:
- "ImageNet Classification with Deep CNNs" (AlexNet, 2012)
- First major use of ReLU

### ReLU6:
- "MobileNets" (Google, 2017)
- Specifically designed for mobile

### Swish:
- "Searching for Activation Functions" (Google Brain, 2017)
- Discovered through neural architecture search

### GELU:
- "Gaussian Error Linear Units" (2016)
- Used in BERT, GPT

## ğŸ¯ à¸ªà¸£à¸¸à¸›

| Use Case | Recommended Activation |
|----------|----------------------|
| ğŸ–¼ï¸ **General CNN** | ReLU |
| ğŸ“± **Mobile/Edge** | ReLU6 |
| ğŸ¤– **Transformers/NLP** | GELU |
| ğŸ® **GAN** | Leaky ReLU |
| ğŸ”¬ **LSTM/RNN** | Tanh + Sigmoid |
| ğŸ† **SOTA CNN** | Swish |
| ğŸ“Š **Binary Classification** | Sigmoid (output) |

## ğŸ’» Testing

```bash
# Compile with activation functions
iverilog -g2012 -o act_test activation_functions.v activation_test.v
vvp act_test

# Test all activation types
# Results will show hardware vs software accuracy
```

---
**Perfect for:** Neural network acceleration on FPGA!
**Created:** November 15, 2025
